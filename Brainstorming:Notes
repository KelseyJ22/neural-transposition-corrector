Goal: learn how to “read” with transposed internal letters

Resources:
Attention is All You Need https://arxiv.org/abs/1706.03762
Visual word Representation http://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(13)00168-X
Deep Spelling https://machinelearnings.co/deep-spelling-9ffef96a24f6
NN Spell Correcting http://www.aclweb.org/anthology/C98-2241


Implementation ideas:
This is roughly analogous to a spell checker
Implement via seq2seq with in = misspelled and out = correctly spelled
But this is almost certainly not how the brain processes things
Better/more interesting:
Somehow learn word representations with specific minimal feature engineering, ie first letter, last letter, length, etc
Question = how little information can it still learn from?
CNN with max pooling across adjacent positions

Plan:
Build baseline that is character-level spell corrector seq2seq
How good is it at this kind of correction?
How much training time/data is required?
Try training same model with fewer but more specific features (first letter, last letter, length tuple)
Want to see if it can learn this pattern more quickly
Intuition = human brain does not actually consider all kinds of misspellings equally, so this model shouldn’t do so either
Attempt CNN with max pooling across adjacent positions
What does this mean exactly?
Dataset: lots of words (Wikipedia) with tuned modifications manufactured via very simple script


- word-level RNN for disambiguation between words with equal probability
- logit for each character = ~0.9 for current letter plus ~0.3 for each neighbor, 0.0 if start/end (these numbers are estimates—need to experiment)
- put that logit through softmax to make the total probability sum to one
- find corpus with relatively small vocab and focus on words of moderate length (under four characters = not interesting, more than maybe eight = computationally complex)
- manufacture the errors I want to test on

--> key idea: rather than using (just) a word embedding, want to learn a representation of the word on the character level that considers each character in turn and use that as the input to the word-level RNN
--> questions:
	How to do this with weighting for neighboring characters rather than just one character at a time?
	Do I want to use pretrained word vectors at all?
	Should I use an RNN or CNN autoencoder for the word level?

Resources

https://arxiv.org/pdf/1611.02344.pdf
https://github.com/atpaino/deep-text-corrector
https://web.stanford.edu/class/cs224n/reports/2762063.pdf
https://github.com/Conchylicultor/DeepQA